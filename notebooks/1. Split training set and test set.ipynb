{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training and test set\n",
    "This notebook split the dataset in training set and test set.\n",
    "\n",
    "The input is taken from raw/inquiry_lessons.\n",
    "\n",
    "The output is exported to data/tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and path definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "\n",
    "root_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "# to import src is necessary to append the root_path to the path\n",
    "sys.path.append(root_path)\n",
    "\n",
    "from collections import Counter\n",
    "from src import topic_preprocessing as topic_pp\n",
    "from src import utterance_proccesing as up\n",
    "\n",
    "raw_path = os.path.join(root_path,'raw')\n",
    "data_path = os.path.join(root_path,'data')\n",
    "pickle_path = os.path.join(raw_path,'pickles')\n",
    "results_path = os.path.join(root_path,'results')\n",
    "transcriptions_path = os.path.join(raw_path,'inquiry_lessons')\n",
    "tables_path = os.path.join(data_path,'tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.utterance_proccesing' from 'C:\\\\Users\\\\CATALINA ESPINOZA\\\\Documents\\\\ciae\\\\Classification_IBL\\\\src\\\\utterance_proccesing.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WITH_STEMMING = True\n",
    "REMOVE_STOPWORDS = True\n",
    "MINIMUM_WORDS_PER_PHRASE = 0\n",
    "GROUP = -1\n",
    "SEED = 10\n",
    "NUM_TOPICS = 60\n",
    "VERSION = 2\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load excels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Group12_2016_No_scaffold.xlsx\n",
      "Loaded Group2_2017_Scaffold1.xlsx\n",
      "Loaded Group3C_2017_Scaffold2.xlsx\n",
      "Loaded Group3_2016_No_scaffold.xlsx\n",
      "Loaded Group3_2017_Scaffold2.xlsx\n",
      "Loaded Group4B_2017_Scaffold2.xlsx\n",
      "Loaded Group4_2017_Scaffold1.xlsx\n",
      "Loaded Group5_2016_No_scaffold.xlsx\n",
      "Loaded Group5_2017_Scaffold2.xlsx\n",
      "Loaded Group6A_2017_Scaffold1.xlsx\n",
      "Loaded Group9_2016_No_scaffold.xlsx\n"
     ]
    }
   ],
   "source": [
    "lessons = []\n",
    "phases = []\n",
    "utterances_percentage = []\n",
    "group = []\n",
    "groups = []\n",
    "groups_dict = {}\n",
    "if GROUP < 0:\n",
    "    for a_lesson in os.listdir(transcriptions_path):\n",
    "        if a_lesson.startswith('~'):\n",
    "            continue\n",
    "        groups_dict[a_lesson] = {}\n",
    "        a_path = os.path.join(transcriptions_path,a_lesson)\n",
    "        df = pd.read_excel(a_path)\n",
    "        try:\n",
    "            lessons.append(df['Utterance'].values)\n",
    "            phases.append(df['Phase'].values)       \n",
    "            ut_numbers = list(df.index)\n",
    "            ut_order = list(map(lambda x:x*1.0/len(ut_numbers),ut_numbers))\n",
    "            utterances_percentage.append(ut_order)\n",
    "            group.append(a_lesson)\n",
    "            groups+= [a_lesson for i in df['Phase'].values]\n",
    "            groups_dict[a_lesson]['phases'] = df['Phase'].values\n",
    "            groups_dict[a_lesson]['utterances'] = [v for v in df['Utterance'].values if v==v]\n",
    "            groups_dict[a_lesson]['ut_order'] = ut_order\n",
    "            print(\"Loaded {}\".format(a_lesson))\n",
    "        except:\n",
    "            lessons.append(df['Unnamed: 7'].values)\n",
    "            phases.append(df['Phase'].values)\n",
    "            ut_numbers = list(df.index)\n",
    "            utterances_percentage.append(list(map(lambda x:x*1.0/len(ut_numbers),ut_numbers)))\n",
    "            group.append(a_lesson)\n",
    "            print(\"Loaded {}\".format(a_lesson))\n",
    "else:\n",
    "    print(\"Error {}\".format(a_lesson))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Group6A_2017_Scaffold1.xlsx', 'Group12_2016_No_scaffold.xlsx']\n"
     ]
    }
   ],
   "source": [
    "test_set_length = 2\n",
    "test_set = random.sample(groups_dict.keys(),test_set_length)\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "train_set = []\n",
    "for i in groups_dict.keys():\n",
    "    if i in test_set:\n",
    "        continue\n",
    "    train_set.append(i)\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Group2_2017_Scaffold1.xlsx',\n",
       " 'Group3C_2017_Scaffold2.xlsx',\n",
       " 'Group3_2016_No_scaffold.xlsx',\n",
       " 'Group3_2017_Scaffold2.xlsx',\n",
       " 'Group4B_2017_Scaffold2.xlsx',\n",
       " 'Group4_2017_Scaffold1.xlsx',\n",
       " 'Group5_2016_No_scaffold.xlsx',\n",
       " 'Group5_2017_Scaffold2.xlsx',\n",
       " 'Group9_2016_No_scaffold.xlsx']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in groups_dict:\n",
    "    clean_utterances = [topic_pp.finnish_preprocessing(x) for x in groups_dict[g]['utterances']]\n",
    "    clean_utterances = [topic_pp.preprocessing(x,REMOVE_STOPWORDS,WITH_STEMMING) for x in clean_utterances]\n",
    "    groups_dict[g]['clean_utterances'] = clean_utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for g in groups_dict:\n",
    "    for element in groups_dict[g]['clean_utterances']:\n",
    "        for word in element:\n",
    "            words.append(word)\n",
    "dict_with_fq = dict(Counter(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_unfrequent = ''\n",
    "for g in groups_dict:\n",
    "    try:\n",
    "        utterances = list(groups_dict[g]['clean_utterances'])\n",
    "        copy_phases = list(groups_dict[g]['phases'])\n",
    "        copy_ut_order = list(groups_dict[g]['ut_order'])\n",
    "        groups_dict[g]['clean_utterances'] = []\n",
    "        groups_dict[g]['ut_order'] = []\n",
    "        groups_dict[g]['phases'] = []\n",
    "        for i,phrase in enumerate(utterances):\n",
    "            a_phrase = []\n",
    "            for w in phrase:\n",
    "                aux_w = w\n",
    "                if dict_with_fq[w]<=1:\n",
    "                    if tag_unfrequent == '':\n",
    "                        continue\n",
    "                    else:\n",
    "                        aux_w = tag_unfrequent\n",
    "                a_phrase.append(aux_w)\n",
    "            if a_phrase != [] or len(a_phrase) != a_phrase.count(''):\n",
    "                groups_dict[g]['clean_utterances'].append(a_phrase)\n",
    "                groups_dict[g]['ut_order'].append(copy_ut_order[i])\n",
    "                groups_dict[g]['phases'].append(copy_phases[i])\n",
    "    except:\n",
    "        print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load lda model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CATALINA ESPINOZA\\AppData\\Local\\conda\\conda\\envs\\teacher_topic_model\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "a_name = 'lda_textbooks_chunksize_alpha_auto_v{}_seed_{}_{}_{}_{}.pickle'.format(VERSION,SEED,NUM_TOPICS,REMOVE_STOPWORDS,WITH_STEMMING)\n",
    "model_file = os.path.join(results_path,'lda_models',a_name)\n",
    "with open(model_file,'rb') as f:\n",
    "    ldamodel = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary length removing unfrequent words: 12960\n"
     ]
    }
   ],
   "source": [
    "dict_file = os.path.join(pickle_path,'dictionary_v{}_seed_{}_{}_{}_{}.pickle'.format(VERSION,SEED,NUM_TOPICS,REMOVE_STOPWORDS,WITH_STEMMING))\n",
    "with open(dict_file,'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "print(\"Dictionary length removing unfrequent words: {}\".format(len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_distribution_phrase(a_phrase):\n",
    "    bow = dictionary.doc2bow(a_phrase)\n",
    "    T = ldamodel.get_document_topics(bow,minimum_probability=0,minimum_phi_value=0.001)\n",
    "    return [x[1] for x in T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description training and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key 1, total 403\n",
      "key 2, total 175\n",
      "key 3, total 406\n",
      "key 4, total 62\n",
      "key 5, total 554\n"
     ]
    }
   ],
   "source": [
    "the_keys = range(1,6)\n",
    "phases_n = [0 for n in range(len(the_keys))]\n",
    "for g in groups_dict:\n",
    "    if g in train_set:\n",
    "        for k in the_keys:\n",
    "            n = groups_dict[g]['phases'].count(k)\n",
    "            phases_n[k-1]+= n\n",
    "for k in the_keys:\n",
    "    print(\"key {}, total {}\".format(k,phases_n[k-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1602"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "403+175+407+62+555"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key 1, total 72\n",
      "key 2, total 33\n",
      "key 3, total 70\n",
      "key 4, total 8\n",
      "key 5, total 98\n"
     ]
    }
   ],
   "source": [
    "the_keys = range(1,6)\n",
    "phases_n = [0 for n in range(len(the_keys))]\n",
    "for g in groups_dict:\n",
    "    if g in test_set:\n",
    "        for k in the_keys:\n",
    "            n = groups_dict[g]['phases'].count(k)\n",
    "            phases_n[k-1]+= n\n",
    "for k in the_keys:\n",
    "    print(\"key {}, total {}\".format(k,phases_n[k-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build table with utterances, topics distribution and phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build table with train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = up.build_simple_df(groups_dict,train_set,ldamodel,dictionary)\n",
    "file_name = '[train]IBL_topic_distribution_by_utterance_minimum_5_words_with_stemming_{}_{}.xlsx'.format(WITH_STEMMING,NUM_TOPICS)\n",
    "df.to_excel(os.path.join(tables_path,'train',file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in train_set:\n",
    "    df = up.build_simplest_df(groups_dict,[group],ldamodel,dictionary)\n",
    "    file_name = '[train]{}_{}_{}.xlsx'.format(group,WITH_STEMMING,NUM_TOPICS)\n",
    "    df.to_excel(os.path.join(tables_path,'fv_62_removing_more_stopwords','train',file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build table with test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = up.build_simple_df(groups_dict,[test_set[0]],ldamodel,dictionary)\n",
    "file_name = '[test1]IBL_topic_distribution_by_utterance_minimum_5_words_with_stemming_{}_{}.xlsx'.format(WITH_STEMMING,NUM_TOPICS)\n",
    "df.to_excel(os.path.join(tables_path,'test',file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = up.build_simple_df(groups_dict,[test_set[1]],ldamodel,dictionary)\n",
    "file_name = '[test2]IBL_topic_distribution_by_utterance_minimum_5_words_with_stemming_{}_{}.xlsx'.format(WITH_STEMMING,NUM_TOPICS)\n",
    "df.to_excel(os.path.join(tables_path,'test',file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in test_set:\n",
    "    df = up.build_simplest_df(groups_dict,[group],ldamodel,dictionary)\n",
    "    file_name = '[test]{}_{}_{}.xlsx'.format(group,WITH_STEMMING,NUM_TOPICS)\n",
    "    df.to_excel(os.path.join(tables_path,'fv_62_removing_more_stopwords','test',file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build excel with before and after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = up.build_next_utterances_df(groups_dict,train_set,ldamodel,dictionary)\n",
    "file_name = '[train]IBL_topic_distribution_by_utterance_with_phrase_before_and_after_time_utterance_minimum_0_words_with_stemming_{}_{}.xlsx'.format(WITH_STEMMING,NUM_TOPICS)\n",
    "df.to_excel(os.path.join(tables_path,'train',file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in train_set:\n",
    "    df = up.build_simplest_next_utterances_df(groups_dict,[group],ldamodel,dictionary)\n",
    "    file_name = '[train]{}_{}_{}.xlsx'.format(group,WITH_STEMMING,NUM_TOPICS)\n",
    "    df.to_excel(os.path.join(tables_path,'fv_182_removing_more_stopwords','train',file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = up.build_next_utterances_df(groups_dict,[test_set[0]],ldamodel,dictionary)\n",
    "file_name = '[test1]IBL_topic_distribution_by_utterance_before_after_{}_{}.xlsx'.format(WITH_STEMMING,NUM_TOPICS)\n",
    "df.to_excel(os.path.join(tables_path,'test',file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = up.build_next_utterances_df(groups_dict,[test_set[1]],ldamodel,dictionary)\n",
    "file_name = '[test2]IBL_topic_distribution_by_utterance_before_after_{}_{}.xlsx'.format(WITH_STEMMING,NUM_TOPICS)\n",
    "df.to_excel(os.path.join(tables_path,'test',file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in test_set:\n",
    "    df = up.build_simplest_next_utterances_df(groups_dict,[group],ldamodel,dictionary)\n",
    "    file_name = '[test]{}_{}_{}.xlsx'.format(group,WITH_STEMMING,NUM_TOPICS)\n",
    "    df.to_excel(os.path.join(tables_path,'fv_182_removing_more_stopwords','test',file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build excel with co-occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = up.build_co_occurrence_df(groups_dict,train_set,ldamodel,dictionary)\n",
    "file_name = '[train]IBL_topic_distribution_by_utterance_with_co_occurrence_time_utterance_minimum_0_words_with_stemming_{}_{}.xlsx'.format(WITH_STEMMING,NUM_TOPICS)\n",
    "df.to_excel(os.path.join(tables_path,'train',file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = up.build_co_occurrence_df(groups_dict,[test_set[0]],ldamodel,dictionary)\n",
    "file_name = '[test1]IBL_topic_distribution_by_utterance_with_co_occurrence_time_utterance_minimum_0_words_with_stemming_{}_{}.xlsx'.format(WITH_STEMMING,NUM_TOPICS)\n",
    "df.to_excel(os.path.join(tables_path,'test',file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = up.build_co_occurrence_df(groups_dict,[test_set[1]],ldamodel,dictionary)\n",
    "file_name = '[test2]IBL_topic_distribution_by_utterance_with_co_occurrence_time_utterance_minimum_0_words_with_stemming_{}_{}.xlsx'.format(WITH_STEMMING,NUM_TOPICS)\n",
    "df.to_excel(os.path.join(tables_path,'test',file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build excel with windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = up.build_windows_df(groups_dict,train_set,ldamodel,dictionary,3)\n",
    "file_name = '[train]IBL_topic_distribution_by_utterance_windows_time_utterance_minimum_0_words_with_stemming_{}_{}.xlsx'.format(WITH_STEMMING,NUM_TOPICS)\n",
    "df.to_excel(os.path.join(tables_path,'train',file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = up.build_windows_df(groups_dict,[test_set[0]],ldamodel,dictionary)\n",
    "file_name = '[test1]IBL_topic_distribution_by_utterance_with_co_occurrence_time_utterance_minimum_0_words_with_stemming_{}_{}.xlsx'.format(WITH_STEMMING,NUM_TOPICS)\n",
    "df.to_excel(os.path.join(tables_path,'test',file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = up.build_windows_df(groups_dict,train_set,ldamodel,dictionary,3)\n",
    "file_name = '[train]IBL_topic_distribution_by_utterance_windows_time_utterance_minimum_0_words_with_stemming_{}_{}.xlsx'.format(WITH_STEMMING,NUM_TOPICS)\n",
    "df.to_excel(os.path.join(tables_path,'train',file_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
